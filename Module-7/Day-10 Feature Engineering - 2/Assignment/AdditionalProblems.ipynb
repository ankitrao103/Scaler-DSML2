{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Scaler Trouble\n",
    "\n",
    "Consider a dataset with two features: 'Weight' and 'Height’.\n",
    "\n",
    "'Weight' values range from 50 to 90, and\n",
    "'Height' values range from 160 to 190.\n",
    "Data:\n",
    "```python\n",
    "Weight = [70, 80, 60, 90, 50],  \n",
    "Height = [170, 180, 160, 190, 165]\n",
    "```\n",
    "Which scaler should we choose to preprocess these features for our machine-learning model?\n",
    "\n",
    "A. Min-Max Scaler for 'Weight' and Standard Scaler for 'Height'\n",
    "B. Standard Scaler for 'Weight' and Min-Max Scaler for 'Height'\n",
    "C. Min-Max Scaler for both 'Weight' and 'Height'\n",
    "D. Standard Scaler for both 'Weight' and 'Height'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Option: D. Standard Scaler for both ‘Weight’ and ‘Height’\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "**Standard Scaler** standardizes data by subtracting the mean and dividing by the standard deviation. This is generally preferred for machine learning models as it:\n",
    "\n",
    "- Makes all features have zero mean and unit variance, which can improve model performance.\n",
    "- Ensures all features contribute equally to the model, regardless of their original units or scales.\n",
    "\n",
    "**Min-Max Scaler** scales the data to a specific range, typically between 0 and 1. This may be useful in certain cases, but it can be problematic for machine learning models because:\n",
    "\n",
    "- It removes information about the spread of the data (variance), which can be important for certain models.\n",
    "- It can amplify the effect of outliers.\n",
    "- In this particular case, both ‘Weight’ and ‘Height’ are continuous features with a well-defined range.\n",
    "\n",
    "Therefore, using Standard Scaler ensures that both features are standardized and contribute equally to the machine learning model.\n",
    "\n",
    "**A and B are incorrect because:**\n",
    "\n",
    "Applying different scaling methods to different features can lead to inconsistencies in the data and potentially harm the performance of the machine learning model.\n",
    "In this case, there is no specific reason to choose Min-Max Scaler over Standard Scaler for either feature.\n",
    "\n",
    "**C is incorrect because:**\n",
    "\n",
    "While Min-Max Scaler can be useful in certain situations, it is not the best approach for this specific case as explained above. Standard Scaler is a more general-purpose scaler and is preferred for machine-learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Outlier Trouble\n",
    "\n",
    "A dataset contains exam scores for students, ranging from 40 to 100. However, there are a few outliers with scores exceeding 120.\n",
    "\n",
    "Which method is more suitable for identifying outliers in this scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Option: Interquartile Range (IQR)\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "**IQR method:**\n",
    "\n",
    "- This method identifies outliers based on the quartiles of the data.\n",
    "- It is more resistant to outliers than the mean-based methods like Z-score, making it better suited for skewed data like exam scores where a few high scores can significantly impact the mean.\n",
    "\n",
    "**Z-score method:**\n",
    "\n",
    "- This method identifies outliers based on their standard deviation from the mean.\n",
    "- However, it can be influenced by outliers itself, leading to inaccurate outlier detection when the data is skewed.\n",
    "- In this scenario, the presence of a few high scores can inflate the standard deviation, potentially masking other outliers or misidentifying valid data points as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Imputer works 2\n",
    "\n",
    "What does the following code snippet do?\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(strategy='most_frequent')\n",
    "imp_mean.fit(data)\n",
    "imputed_train_df = imp_mean.transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct option: Calculates the most frequent value among the non-missing values in a column and then replacing the missing values within each column separately.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- **SimpleImputer()** replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column or using a constant value.\n",
    "\n",
    "- when **strategy= 'most_frequent'** is passed inside, it calculates the mode of the non-missing values in a column and then replaces the missing values within each column separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Handling Categorical features\n",
    "\n",
    "Which of the following statements are true about handling categorical values?\n",
    "\n",
    "\n",
    "a) One-hot encoding is the most effective approach for handling all categorical data.\n",
    "\n",
    "b) One-hot encoding increases the dimensionality of the data.\n",
    "\n",
    "c) Target encoding uses the target variable to estimate the value of each category in a categorical variable.\n",
    "\n",
    "d) Label encoding can be useful for features with a large number of categories.\n",
    "\n",
    "e) No encoding is also sometimes the best option for handling categorical variables.\n",
    "\n",
    "f) Label encoding assigns arbitrary numerical values to categories in a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Option: b, c, f\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "The true statements about handling categorical values are:\n",
    "\n",
    "b) One-hot encoding increases the dimensionality of the data. This is because every category in the original variable is represented by a new binary feature in the one-hot encoded data.\n",
    "\n",
    "c) Target encoding uses the target variable to estimate the value of each category in a categorical variable. This is done by calculating the average target value for each category and using that value to represent the category.\n",
    "\n",
    "f) Label encoding assigns arbitrary numerical values to categories in a categorical variable. This is done by assigning a - unique integer to each category, but the order of the integers does not necessarily reflect the order of the categories.\n",
    "The incorrect statements are:\n",
    "\n",
    "a) One-hot encoding is the most effective approach for handling all categorical data.\n",
    "- This is not true, as other encoding techniques, such as label encoding and target encoding, may be more suitable for certain types of categorical data.\n",
    "\n",
    "d) Label encoding can be useful for features with a large number of categories.\n",
    "- This is not true because when there are many categories, label encoding can create a large number of features, which can increase the dimensionality of the data and make it difficult for the model to learn.\n",
    "\n",
    "e) No encoding is also sometimes the best option for handling categorical variables:\n",
    "- Not true.\n",
    "- Encoding is generally necessary for machine learning models to interpret categorical data correctly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
